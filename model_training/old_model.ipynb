{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9a9e6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import cv2\n",
    "\n",
    "from sklearn.utils import resample\n",
    "import albumentations as A\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dbfdb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found training labels\n",
      "Found testing labels\n",
      "Training data:  25331\n",
      "Testing data:  8238\n",
      "Training labels:  25331\n",
      "Testing labels:  8238\n"
     ]
    }
   ],
   "source": [
    "# -isic_2019\n",
    "#   -ISIC_2019_Training_Input\n",
    "#       -ISIC_2019_Training_GroundTruth.csv\n",
    "#       -ISIC_2019_Training_Input\n",
    "#           -images\n",
    "#   -ISIC_2019_Test_Input\n",
    "#       -ISIC_2019_Test_GroundTruth.csv\n",
    "#       -ISIC_2019_Test_Input\n",
    "#           -images\n",
    "\n",
    "training_path = \"../isic_2019/ISIC_2019_Training_Input/ISIC_2019_Training_Input\"\n",
    "testing_path = \"../isic_2019/ISIC_2019_Test_Input/ISIC_2019_Test_Input\"\n",
    "\n",
    "training_labels_path = \"../isic_2019/ISIC_2019_Training_Input/ISIC_2019_Training_GroundTruth.csv\"\n",
    "testing_labels_path = \"../isic_2019/ISIC_2019_Test_Input/ISIC_2019_Test_GroundTruth.csv\"\n",
    "\n",
    "training_labels = pd.read_csv(training_labels_path)\n",
    "print(\"Found training labels\")\n",
    "testing_labels = pd.read_csv(testing_labels_path)\n",
    "print(\"Found testing labels\")\n",
    "\n",
    "def load_data(path):\n",
    "    data = []\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            data.append(filename)\n",
    "    return data\n",
    "\n",
    "training_data = load_data(training_path)\n",
    "print(\"Training data: \", len(training_data))\n",
    "testing_data = load_data(testing_path)\n",
    "print(\"Testing data: \", len(testing_data))\n",
    "\n",
    "print(\"Training labels: \", len(training_labels))\n",
    "print(\"Testing labels: \", len(testing_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a55299ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels preview:\n",
      "          image  MEL\n",
      "0  ISIC_0000000  0.0\n",
      "1  ISIC_0000001  0.0\n",
      "2  ISIC_0000002  1.0\n",
      "3  ISIC_0000003  0.0\n",
      "4  ISIC_0000004  1.0\n",
      "Melanoma images: 4522\n",
      "Non-Melanoma images: 20809\n",
      "MEL\n",
      "0.0    82.148356\n",
      "1.0    17.851644\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Verify extracted data\n",
    "\n",
    "\n",
    "df = training_labels\n",
    "print(\"Training labels preview:\")\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "# Peek of MEL column and NON-MEL values\n",
    "melanoma_count = df[df[\"MEL\"] == 1].shape[0]\n",
    "non_melanoma_count = df[df[\"MEL\"] == 0].shape[0]\n",
    "\n",
    "# Printing the counts\n",
    "print(f\"Melanoma images: {melanoma_count}\")\n",
    "print(f\"Non-Melanoma images: {non_melanoma_count}\")\n",
    "\n",
    "# Get overall class distribution\n",
    "print(df[\"MEL\"].value_counts(normalize=True) * 100)  # Shows % distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2f29db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''FILTERING'''\n",
    "# Hair removal\n",
    "def remove_hair(img):\n",
    "    #Black hat filter\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(7,17))\n",
    "    blackhat = cv2.morphologyEx(img, cv2.MORPH_BLACKHAT, kernel)\n",
    "    #Gaussian filter\n",
    "    bhg= cv2.GaussianBlur(blackhat,(3,3),cv2.BORDER_DEFAULT)\n",
    "    #Binary thresholding (MASK)\n",
    "    ret,mask = cv2.threshold(bhg,10,255,cv2.THRESH_BINARY)\n",
    "    #Replace pixels of the mask\n",
    "    dst = cv2.inpaint(img,mask,7,cv2.INPAINT_TELEA)\n",
    "    # plt.imshow(dst)\n",
    "    # plt.show\n",
    "    return dst\n",
    "    \n",
    "# Gray scale\n",
    "def convert_to_grayscale(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "# Noise reduction\n",
    "def reduce_noise(image):\n",
    "    bilateral = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "    median = cv2.medianBlur(bilateral, 5)\n",
    "    return median\n",
    "    \n",
    "# Contrast enhancement\n",
    "def enhance_contrast(image):\n",
    "    enhanced_img = (cv2.createCLAHE(clipLimit=2, tileGridSize=(8,8))).apply(image)\n",
    "    return enhanced_img\n",
    "\n",
    "# Resizing\n",
    "def resize_img(img, size=(224, 224)):\n",
    "    resized_img = cv2.resize(img, size)\n",
    "    return resized_img\n",
    "\n",
    "# Other filters (need to debug)\n",
    "\n",
    "# Edge detection\n",
    "def segment_lesion(image):\n",
    "        \n",
    "    #https://docs.opencv.org/4.x/d7/d4d/tutorial_py_thresholding.html \n",
    "    # look at Otsu binarization; very nice\n",
    "    _, thresh = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    \n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "    mask = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    return mask\n",
    "\n",
    "def enhance_borders(image):\n",
    "\n",
    "    # https://docs.opencv.org/4.x/d2/d2c/tutorial_sobel_derivatives.html\n",
    "    sobelx = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    sobely = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    gradient = np.sqrt(sobelx**2 + sobely**2)\n",
    "    \n",
    "    gradient = np.uint8(gradient * 255 / gradient.max())\n",
    "    return gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "500e6090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory already exists, deleting\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m         output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, image_name)\n\u001b[0;32m     38\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mimwrite(output_path, preprocessed_image)\n\u001b[1;32m---> 40\u001b[0m \u001b[43mpreprocess_all_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../old_preprocessed_images\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 36\u001b[0m, in \u001b[0;36mpreprocess_all_images\u001b[1;34m(image_list, output_dir)\u001b[0m\n\u001b[0;32m     34\u001b[0m image_name \u001b[38;5;241m=\u001b[39m image_list[image_name]\n\u001b[0;32m     35\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(training_path, image_name)\n\u001b[1;32m---> 36\u001b[0m preprocessed_image \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, image_name)\n\u001b[0;32m     38\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(output_path, preprocessed_image)\n",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m, in \u001b[0;36mpreprocessing\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      5\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m      6\u001b[0m gray \u001b[38;5;241m=\u001b[39m convert_to_grayscale(image)\n\u001b[1;32m----> 7\u001b[0m hair_remove \u001b[38;5;241m=\u001b[39m \u001b[43mremove_hair\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m noise_reduced \u001b[38;5;241m=\u001b[39m reduce_noise(hair_remove)\n\u001b[0;32m     10\u001b[0m contrast \u001b[38;5;241m=\u001b[39m enhance_contrast(noise_reduced)\n",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m, in \u001b[0;36mremove_hair\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     10\u001b[0m ret,mask \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mthreshold(bhg,\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m255\u001b[39m,cv2\u001b[38;5;241m.\u001b[39mTHRESH_BINARY)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#Replace pixels of the mask\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m dst \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minpaint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINPAINT_TELEA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# plt.imshow(dst)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# plt.show\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''PREPROCESSING'''\n",
    "import shutil\n",
    "def preprocessing(image):\n",
    "    image = cv2.imread(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    gray = convert_to_grayscale(image)\n",
    "    hair_remove = remove_hair(gray)\n",
    "    noise_reduced = reduce_noise(hair_remove)\n",
    "    \n",
    "    contrast = enhance_contrast(noise_reduced)\n",
    "\n",
    "    # Very wonky things happen depending on specific images when trying to get specific borders\n",
    "    #mask = segment_lesion(image)   \n",
    "    # borders = enhance_borders(noise_reduced)\n",
    "    #final = cv2.bitwise_and(contrast, contrast, mask=mask)\n",
    "\n",
    "    resized = cv2.resize(contrast, (224,224))\n",
    "    return resized\n",
    "\n",
    "def preprocess_all_images(image_list, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        print(\"Creating output directory\")\n",
    "        os.makedirs(output_dir)\n",
    "    elif os.path.exists(output_dir):\n",
    "        print(\"Output directory already exists, deleting\")\n",
    "        shutil.rmtree(output_dir)\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    i = 0\n",
    "    for image_name in range(len(image_list)): # <------------------------------ Change for full dataset len(image_list)\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Preprocessing image {i}/{len(image_list)}\")\n",
    "        image_name = image_list[image_name]\n",
    "        image_path = os.path.join(training_path, image_name)\n",
    "        preprocessed_image = preprocessing(image_path)\n",
    "        output_path = os.path.join(output_dir, image_name)\n",
    "        cv2.imwrite(output_path, preprocessed_image)\n",
    "\n",
    "preprocess_all_images(training_data, \"../old_preprocessed_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe65734",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''UPDATING LABELS'''\n",
    "\n",
    "def update_labels(labels_path):\n",
    "    df = pd.read_csv(labels_path)\n",
    "    df = df[['image', 'MEL']] # Drop other columns\n",
    "    return df\n",
    "\n",
    "\n",
    "df_train_labels_updated = update_labels(training_labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c7efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''DATA AUGMENTATION'''\n",
    "\n",
    "'''UNDERSAMPLING'''\n",
    "# Simple random undersampling to achieve desired balance\n",
    "# random_state HH 88\n",
    "\n",
    "def undersample_data(df, majority_split, new_labels_path):\n",
    "    majority = df[df[\"MEL\"] == 0]\n",
    "    minority = df[df[\"MEL\"] == 1]\n",
    "\n",
    "    # Calculate size\n",
    "    # major_count = majority.shape[0]\n",
    "    minor_count = minority.shape[0]\n",
    "    size = int(minor_count // (1 - majority_split)) - minor_count\n",
    "\n",
    "    majority_undersampled = resample(majority,\n",
    "                                    replace=False,\n",
    "                                    n_samples=size,\n",
    "                                    random_state=88)\n",
    "    \n",
    "    df_undersampled = pd.concat([majority_undersampled, minority])\n",
    "    df_ordered = df_undersampled.sort_values(by=\"image\").reset_index(drop=True)\n",
    "\n",
    "    # print(df_ordered[df_ordered[\"MEL\"] == 0].shape[0])\n",
    "    # print(df_ordered[df_ordered[\"MEL\"] == 1].shape[0])\n",
    "         \n",
    "\n",
    "    df_ordered.to_csv(new_labels_path, index=False)\n",
    "    #print(len(df_ordered))\n",
    "\n",
    "majority_split = 0.5 # XX% majority (Non-Melanoma), XX% minority (Melanoma)\n",
    "undersample_data(df_train_labels_updated, majority_split, \"../old_preprocessed_images/ISIC_2019_Training_GroundTruth_preprocessed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de9256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_process_images(image_path):\n",
    "    image_path += \".jpg\"\n",
    "    img = load_img(image_path)\n",
    "    img_array = img_to_array(img)\n",
    "\n",
    "    img_array = img_array / 255.0\n",
    "    return img_array\n",
    "\n",
    "\n",
    "def get_all_data(all_images, labels, path):\n",
    "    images = []\n",
    "    all_labels = []\n",
    "\n",
    "    for img, label in zip(all_images, labels):\n",
    "        img_path = os.path.join(path, img).replace(\"\\\\\", \"/\")\n",
    "        img_pixels = load_process_images(img_path)\n",
    "\n",
    "        images.append(img_pixels)\n",
    "        all_labels.append(label)\n",
    "    \n",
    "    return np.array(images), np.array(all_labels)\n",
    "\n",
    "def augment_data():\n",
    "    # who would have thought: https://keras.io/api/layers/preprocessing_layers/image_augmentation/\n",
    "    return tf.keras.Sequential([\n",
    "        layers.RandomRotation(0.2),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomFlip(\"vertical\"),\n",
    "        layers.RandomZoom(0.2),\n",
    "        layers.RandomBrightness(0.2),\n",
    "        layers.RandomContrast(0.2),\n",
    "    ])\n",
    "\n",
    "# old model 81%\n",
    "def build_model():\n",
    "\n",
    "    # random bs\n",
    "    model = models.Sequential([\n",
    "        \n",
    "       layers.Conv2D(32, (3, 3), activation='relu',\n",
    "               padding='same', input_shape=(224, 224, 3)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        # sigmoid here\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def oversample_model():\n",
    "    model = models.Sequential([\n",
    "        \n",
    "        augment_data(),\n",
    "        \n",
    "        layers.Conv2D(32, (3, 3), activation='relu',\n",
    "               padding='same', input_shape=(224, 224, 1)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        # sigmoid here\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1269f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "labels_path = \"../old_preprocessed_images/ISIC_2019_Training_GroundTruth_preprocessed.csv\"\n",
    "images_path = \"../old_preprocessed_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9788234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anony\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Create 80 - 20 train test (validation set) split\n",
    "\n",
    "stratify - Ensures class distribution is similar in both sets based on the labels\n",
    "\n",
    "'''\n",
    "df = pd.read_csv(labels_path)\n",
    "image_names = df['image'].values # X - images\n",
    "image_labels = df['MEL'].values # y - labels\n",
    "\n",
    "X_train_names, X_test_names, y_train, y_test = train_test_split(image_names, image_labels, \n",
    "                                                    test_size=0.2, random_state=88, stratify=image_labels)\n",
    "\n",
    "# print(\"Train set size: \", len(X_train))\n",
    "# print(\"Test set size: \", len(X_test))\n",
    "\n",
    "X_train, y_train = get_all_data(X_train_names, y_train, images_path)\n",
    "\n",
    "X_testing, y_testing = get_all_data(X_test_names, y_test, images_path)\n",
    "\n",
    "model = build_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485cefe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/50\n",
      "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m620s\u001b[0m 3s/step - accuracy: 0.6180 - auc: 0.6601 - loss: 0.8444 - precision: 0.6226 - recall: 0.6025 - val_accuracy: 0.5003 - val_auc: 0.6846 - val_loss: 1.0623 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m201/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1:12\u001b[0m 3s/step - accuracy: 0.7290 - auc: 0.7978 - loss: 0.5467 - precision: 0.7272 - recall: 0.7304"
     ]
    }
   ],
   "source": [
    "#CONSTANTS\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "\n",
    "total = len(y_train)\n",
    "pos_weight = (total / (2 * np.sum(y_train)))\n",
    "neg_weight = (total / (2 * (total - np.sum(y_train))))\n",
    "class_imbal = {0: neg_weight, 1: pos_weight}\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', \n",
    "                tf.keras.metrics.AUC(),\n",
    "                tf.keras.metrics.Precision(),\n",
    "                tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "modeling = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_testing, y_testing),\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    class_weight=class_imbal,\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(modeling.history['loss'], label='Training Loss')\n",
    "plt.plot(modeling.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(modeling.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(modeling.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3721598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('melanoma_undersample_model.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
